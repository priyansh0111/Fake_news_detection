{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJSUOuRm7Hh",
        "outputId": "93054a8f-f3db-4f79-c546-d9e1c3a017db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-IHznJ8mS5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHLmItSbm6EM",
        "outputId": "f1edae00-11c7-43af-9adb-076b0b36d227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version : 2.2.1+cu121\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Fixing the randomness of CUDA.\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"PyTorch Version : {}\".format(torch.__version__))\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28yIwHQvnNxG"
      },
      "outputs": [],
      "source": [
        "worksapce = '/content/drive/MyDrive/BTP-NLP/liar_dataset2/'\n",
        "model_save = 'TC+TC+CB+FZ.pt'\n",
        "model_name = 'TC+TC+CB+FZ'\n",
        "num_epochs = 30\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "num_classes = 6\n",
        "padding_idx = 0\n",
        "metadata_each_dim = 10\n",
        "\n",
        "\n",
        "col = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "\n",
        "label_map = {0: 'pants-fire', 1: 'false', 2: 'barely-true', 3: 'half-true', 4: 'mostly-true', 5: 'true'}\n",
        "label_convert = {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true':5}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qd4BRconZME"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(worksapce + 'train.tsv', sep = '\\t', names = col)\n",
        "test_data = pd.read_csv(worksapce + 'test.tsv', sep = '\\t', names = col)\n",
        "val_data = pd.read_csv(worksapce + 'valid.tsv', sep = '\\t', names = col)\n",
        "\n",
        "# Replace NaN values with 'NaN'\n",
        "train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "train_data.fillna('NaN', inplace=True)\n",
        "\n",
        "test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "test_data.fillna('NaN', inplace=True)\n",
        "\n",
        "val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "val_data.fillna('NaN', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIF8FRGbncKA"
      },
      "outputs": [],
      "source": [
        "def textProcess(input_text, max_length = -1):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    if max_length == -1:\n",
        "        tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "    else:\n",
        "        tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbV2T6y4qEGu"
      },
      "outputs": [],
      "source": [
        "# from transformers import XLNetTokenizer\n",
        "\n",
        "# def textProcess(input_text, max_length=-1):\n",
        "#   tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')  # Adjust casing based on your needs\n",
        "#   if max_length == -1:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "#   else:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "#   return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIw-WJlWWr3H"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset for loading the data\n",
        "class LiarDataset(data.Dataset):\n",
        "    def __init__(self, data_df, statement, label_onehot, label, subject, speaker, job_title, state_info,\n",
        "                     party_affiliation, barely_true_counts, false_counts, half_true_counts, mostly_true_counts,\n",
        "                    pants_on_fire_counts, context):\n",
        "        self.data_df = data_df\n",
        "        self.statement = statement\n",
        "        self.label_onehot = label_onehot\n",
        "        self.label = label\n",
        "        self.metadata_text = torch.cat((subject.int(), speaker.int(), job_title.int(), state_info.int(), party_affiliation.int(),\n",
        "                                   context.int()), dim=-1)\n",
        "        self.metadata_number = torch.cat((torch.tensor(barely_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(false_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(half_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(mostly_true_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(pants_on_fire_counts, dtype=torch.float).unsqueeze(1)), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        statement = self.statement[idx]\n",
        "        label_onehot = self.label_onehot[idx]\n",
        "        label = self.label[idx]\n",
        "        metadata_text = self.metadata_text[idx]\n",
        "        metadata_number = self.metadata_number[idx]\n",
        "        return statement, label_onehot, label, metadata_text, metadata_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF7lM401WuZM"
      },
      "outputs": [],
      "source": [
        "# Define the data loaders for training and validation\n",
        "train_text = torch.tensor(textProcess(train_data['statement'].tolist())['input_ids'])\n",
        "train_label = torch.nn.functional.one_hot(torch.tensor(train_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "train_subject = torch.tensor(textProcess(train_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_speaker = torch.tensor(textProcess(train_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_job_title = torch.tensor(textProcess(train_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_state_info = torch.tensor(textProcess(train_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_party_affiliation = torch.tensor(textProcess(train_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_context = torch.tensor(textProcess(train_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "train_dataset = LiarDataset(train_data, train_text, train_label, torch.tensor(train_data['label'].replace(label_convert)),\n",
        "                            train_subject, train_speaker, train_job_title,\n",
        "                            train_state_info, train_party_affiliation,\n",
        "                            train_data['barely_true_counts'].tolist(), train_data['false_counts'].tolist(),\n",
        "                            train_data['half_true_counts'].tolist(), train_data['mostly_true_counts'].tolist(),\n",
        "                            train_data['pants_on_fire_counts'].tolist(), train_context)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_text = torch.tensor(textProcess(val_data['statement'].tolist())['input_ids'])\n",
        "val_label = torch.nn.functional.one_hot(torch.tensor(val_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "val_subject = torch.tensor(textProcess(val_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_speaker = torch.tensor(textProcess(val_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_job_title = torch.tensor(textProcess(val_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_state_info = torch.tensor(textProcess(val_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_party_affiliation = torch.tensor(textProcess(val_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_context = torch.tensor(textProcess(val_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "val_dataset = LiarDataset(val_data, val_text, val_label, torch.tensor(val_data['label'].replace(label_convert)),\n",
        "                          val_subject, val_speaker, val_job_title,\n",
        "                          val_state_info, val_party_affiliation,\n",
        "                          val_data['barely_true_counts'].tolist(), val_data['false_counts'].tolist(),\n",
        "                          val_data['half_true_counts'].tolist(), val_data['mostly_true_counts'].tolist(),\n",
        "                          val_data['pants_on_fire_counts'].tolist(), val_context)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "test_text = torch.tensor(textProcess(test_data['statement'].tolist())['input_ids'])\n",
        "test_label = torch.nn.functional.one_hot(torch.tensor(test_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "test_subject = torch.tensor(textProcess(test_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_speaker = torch.tensor(textProcess(test_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_job_title = torch.tensor(textProcess(test_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_state_info = torch.tensor(textProcess(test_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_party_affiliation = torch.tensor(textProcess(test_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_context = torch.tensor(textProcess(test_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "test_dataset = LiarDataset(test_data, test_text, test_label, torch.tensor(test_data['label'].replace(label_convert)),\n",
        "                          test_subject, test_speaker, test_job_title,\n",
        "                          test_state_info, test_party_affiliation,\n",
        "                          test_data['barely_true_counts'].tolist(), test_data['false_counts'].tolist(),\n",
        "                          test_data['half_true_counts'].tolist(), test_data['mostly_true_counts'].tolist(),\n",
        "                          test_data['pants_on_fire_counts'].tolist(), test_context)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lgk9vPy6ahI",
        "outputId": "637c6ea0-b4a9-4cb9-af83-81dc331469d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10240, 512])\n",
            "torch.Size([1267, 512])\n",
            "torch.Size([1284, 77])\n"
          ]
        }
      ],
      "source": [
        "print(train_text.size())\n",
        "print(test_text.size())\n",
        "print(val_text.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoWCmsQLWzms"
      },
      "outputs": [],
      "source": [
        "class FuzzyLayer(nn.Module):\n",
        "    def __init__(self, input_dim, membership_num):\n",
        "        super(FuzzyLayer, self).__init__()\n",
        "\n",
        "        # input_dim: feature number of the dataset\n",
        "        # membership_num: number of membership function, also known as the class number\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.membership_num = membership_num\n",
        "\n",
        "        self.membership_miu = nn.Parameter(torch.Tensor(self.membership_num, self.input_dim).to(DEVICE), requires_grad=True)\n",
        "        self.membership_sigma = nn.Parameter(torch.Tensor(self.membership_num, self.input_dim).to(DEVICE), requires_grad=True)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.membership_miu)\n",
        "        nn.init.ones_(self.membership_sigma)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        batch_size = input_seq.size()[0]\n",
        "        input_seq_exp = input_seq.unsqueeze(1).expand(batch_size, self.membership_num, self.input_dim)\n",
        "        membership_miu_exp = self.membership_miu.unsqueeze(0).expand(batch_size, self.membership_num, self.input_dim)\n",
        "        membership_sigma_exp = self.membership_sigma.unsqueeze(0).expand(batch_size, self.membership_num, self.input_dim)\n",
        "\n",
        "        fuzzy_membership = torch.mean(torch.exp((-1 / 2) * ((input_seq_exp - membership_miu_exp) / membership_sigma_exp) ** 2), dim=-1)\n",
        "        return fuzzy_membership"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdTl-j0jW4qp"
      },
      "outputs": [],
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = embedding_dim,\n",
        "                                              out_channels = n_filters,\n",
        "                                              kernel_size = fs)\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        #text = [batch size, sent len]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        #pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIbGgXVqmmvH"
      },
      "source": [
        "### P1 - Trying Capsule network code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duDEGeZ5l9xu",
        "outputId": "2bd32646-b679-4b28-9c02-60e7d0e59a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/pytorch/tnt.git@master\n",
            "  Cloning https://github.com/pytorch/tnt.git (to revision master) to /tmp/pip-req-build-4sqhv030\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/tnt.git /tmp/pip-req-build-4sqhv030\n",
            "  Resolved https://github.com/pytorch/tnt.git to commit f9298967f027bba081e289208b9c24361eaa77b5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (1.25.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (2.15.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (5.9.5)\n",
            "Requirement already satisfied: pyre-extensions in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (0.0.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (4.66.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from torchtnt==0.2.3) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions->torchtnt==0.2.3) (0.9.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (2.31.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt==0.2.3) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt==0.2.3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtnt==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt==0.2.3) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt==0.2.3) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt==0.2.3) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->torchtnt==0.2.3) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt==0.2.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt==0.2.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt==0.2.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt==0.2.3) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt==0.2.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtnt==0.2.3) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions->torchtnt==0.2.3) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt==0.2.3) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->torchtnt==0.2.3) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/pytorch/tnt.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOjAhPCmKaQ",
        "outputId": "3b2aae7c-692a-4a4e-81c6-eba1f4dd9a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-nlp==0.4.1 in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp==0.4.1) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp==0.4.1) (2.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp==0.4.1) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp==0.4.1) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pytorch-nlp==0.4.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pytorch-nlp==0.4.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pytorch-nlp==0.4.1) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-nlp==0.4.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-nlp==0.4.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-nlp==0.4.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-nlp==0.4.1) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->pytorch-nlp==0.4.1) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-nlp==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1_SqR1gmPrc",
        "outputId": "24c8949c-95eb-4a16-b18e-52cb9be6f63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/leftthomas/CapsuleLayer.git@master\n",
            "  Cloning https://github.com/leftthomas/CapsuleLayer.git (to revision master) to /tmp/pip-req-build-lb8v0q_s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/leftthomas/CapsuleLayer.git /tmp/pip-req-build-lb8v0q_s\n",
            "  Resolved https://github.com/leftthomas/CapsuleLayer.git to commit 2205a603ac1cb3d24673ea7796a4a7e817914712\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from capsule-layer==0.5.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->capsule-layer==0.5.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->capsule-layer==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->capsule-layer==0.5.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->capsule-layer==0.5.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/leftthomas/CapsuleLayer.git@master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q78Vi2Ecm56a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from capsule_layer import CapsuleLinear\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class CompositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_embeddings, embedding_dim, num_codebook, num_codeword=None, num_repeat=10, weighted=True,\n",
        "                 return_code=False):\n",
        "        super(CompositionalEmbedding, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_codebook = num_codebook\n",
        "        self.num_repeat = num_repeat\n",
        "        self.weighted = weighted\n",
        "        self.return_code = return_code\n",
        "\n",
        "        if num_codeword is None:\n",
        "            num_codeword = math.ceil(math.pow(num_embeddings, 1 / num_codebook))\n",
        "        self.num_codeword = num_codeword\n",
        "        self.code = Parameter(torch.Tensor(num_embeddings, num_codebook, num_codeword))\n",
        "        self.codebook = Parameter(torch.Tensor(num_codebook, num_codeword, embedding_dim))\n",
        "\n",
        "        nn.init.normal_(self.code)\n",
        "        nn.init.normal_(self.codebook)\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size = input.size(0)\n",
        "        index = input.view(-1)\n",
        "        code = self.code.index_select(dim=0, index=index)\n",
        "        if self.weighted:\n",
        "            # reweight, do softmax, make sure the sum of weight about each book to 1\n",
        "            code = F.softmax(code, dim=-1)\n",
        "            out = (code[:, :, None, :] @ self.codebook[None, :, :, :]).squeeze(dim=-2).sum(dim=1)\n",
        "        else:\n",
        "            # because Gumbel SoftMax works in a stochastic manner, needs to run several times to\n",
        "            # get more accurate embedding\n",
        "            code = (torch.sum(torch.stack([F.gumbel_softmax(code) for _ in range(self.num_repeat)]), dim=0)).argmax(\n",
        "                dim=-1)\n",
        "            out = []\n",
        "            for index in range(self.num_codebook):\n",
        "                out.append(self.codebook[index, :, :].index_select(dim=0, index=code[:, index]))\n",
        "            out = torch.sum(torch.stack(out), dim=0)\n",
        "            code = F.one_hot(code, num_classes=self.num_codeword)\n",
        "\n",
        "        out = out.view(batch_size, -1, self.embedding_dim)\n",
        "        code = code.view(batch_size, -1, self.num_codebook, self.num_codeword)\n",
        "        if self.return_code:\n",
        "            return out, code\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.num_embeddings) + ', ' + str(self.embedding_dim) + ')'\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_codebook, num_codeword, hidden_size, in_length, out_length,\n",
        "                 num_class, routing_type, embedding_type, classifier_type, num_iterations, num_repeat, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_length, self.out_length = in_length, out_length\n",
        "        self.hidden_size, self.classifier_type = hidden_size, classifier_type\n",
        "        self.embedding_type = embedding_type\n",
        "\n",
        "        if embedding_type == 'cwc':\n",
        "            self.embedding = CompositionalEmbedding(vocab_size, embedding_size, num_codebook, num_codeword,\n",
        "                                                    weighted=True)\n",
        "        elif embedding_type == 'cc':\n",
        "            self.embedding = CompositionalEmbedding(vocab_size, embedding_size, num_codebook, num_codeword, num_repeat,\n",
        "                                                    weighted=False)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.features = nn.GRU(embedding_size, self.hidden_size, num_layers=2, dropout=dropout, batch_first=True,\n",
        "                               bidirectional=True)\n",
        "        if classifier_type == 'capsule' and routing_type == 'k_means':\n",
        "            self.classifier = CapsuleLinear(out_capsules=num_class, in_length=self.in_length,\n",
        "                                            out_length=self.out_length, in_capsules=None, share_weight=True,\n",
        "                                            routing_type='k_means', num_iterations=num_iterations, bias=False)\n",
        "        elif classifier_type == 'capsule' and routing_type == 'dynamic':\n",
        "            self.classifier = CapsuleLinear(out_capsules=num_class, in_length=self.in_length,\n",
        "                                            out_length=self.out_length, in_capsules=None, share_weight=True,\n",
        "                                            routing_type='dynamic', num_iterations=num_iterations, bias=False)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(in_features=self.hidden_size, out_features=num_class, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        out, _ = self.features(embed)\n",
        "\n",
        "        out = out[:, :, :self.hidden_size] + out[:, :, self.hidden_size:]\n",
        "        out = out.mean(dim=1).contiguous()\n",
        "\n",
        "        if self.classifier_type == 'capsule':\n",
        "            out = out.view(out.size(0), -1, self.in_length)\n",
        "            out = self.classifier(out)\n",
        "            classes = out[1].norm(dim=-1)\n",
        "        else:\n",
        "            out = out.view(out.size(0), -1)\n",
        "            classes = self.classifier(out)\n",
        "\n",
        "        return classes\n",
        "\n",
        "        # if self.classifier_type == 'capsule':\n",
        "        #     out = out.view(out.size(0), -1, self.in_length)\n",
        "        #     out = self.classifier(out)\n",
        "        #     out = torch.stack(out)\n",
        "        #     classes = out.norm(dim=-1)\n",
        "        #     classes = classes.norm(dim=0)\n",
        "        # else:\n",
        "        #     out = out.view(out.size(0), -1)\n",
        "        #     classes = self.classifier(out)\n",
        "        # return classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rITtOOrGXCWy"
      },
      "outputs": [],
      "source": [
        "class CNNBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=32, kernel_size=1)\n",
        "        self.rnn = nn.LSTM(32,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, metadata):\n",
        "        #metadata = [batch size, metadata dim]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(metadata))\n",
        "        #embedded = [batch size, metadata dim, emb dim]\n",
        "\n",
        "        embedded = torch.reshape(embedded, (metadata.size(0), 128, 1))\n",
        "\n",
        "        conved = F.relu(self.conv(embedded))\n",
        "        #conved = [batch size, n_filters, metadata dim - filter_sizes[n] + 1]\n",
        "\n",
        "        conved = torch.reshape(conved, (metadata.size(0), 32))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(conved)\n",
        "        #outputs = [metadata dim - filter_sizes[n] + 1, batch size, hid dim * num directions]\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        # hidden = self.dropout(torch.cat((hidden[-1,:], hidden[0,:]), dim = -1))\n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.fc(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j35V7KB_XIFs"
      },
      "outputs": [],
      "source": [
        "class LiarModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx, input_dim, input_dim_metadata, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "\n",
        "        self.textcnn = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx)\n",
        "\n",
        "        self.capsulecnn = Model(vocab_size=vocab_size, embedding_size=embedding_dim, num_codebook=8,\n",
        "                                num_codeword=None, hidden_size=128, in_length=8, out_length=8, num_class=6,\n",
        "                                routing_type='dynamic', embedding_type='cwc', classifier_type='capsule',\n",
        "                                num_iterations=10, num_repeat=10, dropout=0.5)\n",
        "\n",
        "        self.capsulecnn2 = Model(vocab_size=vocab_size, embedding_size=embedding_dim, num_codebook=8,\n",
        "                                num_codeword=None, hidden_size=128, in_length=8, out_length=16, num_class=6,\n",
        "                                routing_type='dynamic', embedding_type='cwc', classifier_type='capsule',\n",
        "                                num_iterations=10, num_repeat=10, dropout=0.5)\n",
        "\n",
        "        self.textcnn2 = TextCNN(vocab_size, input_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx)\n",
        "\n",
        "        self.cnn_bilstm = CNNBiLSTM(input_dim_metadata, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
        "        self.fuzzy = FuzzyLayer(output_dim, output_dim)\n",
        "        self.fuse = nn.Linear(output_dim * 4, output_dim)\n",
        "\n",
        "    def forward(self, text, metadata_text, metadata_number):\n",
        "        #text = [batch size, sent len]\n",
        "        #metadata = [batch size, metadata dim]\n",
        "\n",
        "        # text_output = self.textcnn(text)\n",
        "        text_output = self.capsulecnn(text)\n",
        "\n",
        "        return text_output\n",
        "\n",
        "        # metadata_output_text = self.textcnn2(metadata_text)\n",
        "        metadata_output_text = self.capsulecnn2(metadata_text)\n",
        "\n",
        "        metadata_output_number = self.cnn_bilstm(metadata_number)\n",
        "\n",
        "        metadata_output_fuzzy = self.fuzzy(metadata_output_number)\n",
        "\n",
        "        fused_output = self.fuse(torch.cat((text_output, metadata_output_text, metadata_output_number, metadata_output_fuzzy), dim=1))\n",
        "        return fused_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnE9XZjiXQJe",
        "outputId": "1147f255-cb4f-41be-c44e-4bdf4a652c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Best Result Updated at Epoch 1, Val Loss: 0.9686 *****\n",
            "Epoch [1/30], Time: 11.56s, \n",
            "Train Loss: 0.9707, Train Acc: 0.1936, Train F1 Macro: 0.1608, Train F1 Micro: 0.1936, \n",
            "Val Loss: 0.9686, Val Acc: 0.2048, Val F1 Macro: 0.0567, Val F1 Micro: 0.2048\n",
            "Test Loss: 0.9691, Test Acc: 0.1965, Test F1 Macro: 0.0547, Test F1 Micro: 0.1965\n",
            "***** Best Result Updated at Epoch 2, Val Loss: 0.9680 *****\n",
            "Epoch [2/30], Time: 11.59s, \n",
            "Train Loss: 0.9682, Train Acc: 0.1979, Train F1 Macro: 0.1529, Train F1 Micro: 0.1958, \n",
            "Val Loss: 0.9680, Val Acc: 0.2173, Val F1 Macro: 0.0996, Val F1 Micro: 0.2111\n",
            "Test Loss: 0.9663, Test Acc: 0.1973, Test F1 Macro: 0.0561, Test F1 Micro: 0.1973\n",
            "***** Best Result Updated at Epoch 3, Val Loss: 0.9676 *****\n",
            "Epoch [3/30], Time: 11.80s, \n",
            "Train Loss: 0.9670, Train Acc: 0.2018, Train F1 Macro: 0.1447, Train F1 Micro: 0.1978, \n",
            "Val Loss: 0.9676, Val Acc: 0.2048, Val F1 Macro: 0.0946, Val F1 Micro: 0.2090\n",
            "Test Loss: 0.9671, Test Acc: 0.1965, Test F1 Macro: 0.0547, Test F1 Micro: 0.1965\n",
            "Epoch [4/30], Time: 11.74s, \n",
            "Train Loss: 0.9670, Train Acc: 0.2021, Train F1 Macro: 0.1391, Train F1 Micro: 0.1988, \n",
            "Val Loss: 0.9690, Val Acc: 0.1931, Val F1 Macro: 0.0974, Val F1 Micro: 0.2050\n",
            "Test Loss: 0.9675, Test Acc: 0.1855, Test F1 Macro: 0.0720, Test F1 Micro: 0.1855\n",
            "Epoch [5/30], Time: 11.59s, \n",
            "Train Loss: 0.9666, Train Acc: 0.2044, Train F1 Macro: 0.1358, Train F1 Micro: 0.1999, \n",
            "Val Loss: 0.9679, Val Acc: 0.2056, Val F1 Macro: 0.1200, Val F1 Micro: 0.2051\n",
            "Test Loss: 0.9677, Test Acc: 0.2273, Test F1 Macro: 0.1079, Test F1 Micro: 0.2273\n",
            "Epoch [6/30], Time: 11.51s, \n",
            "Train Loss: 0.9652, Train Acc: 0.2179, Train F1 Macro: 0.1362, Train F1 Micro: 0.2029, \n",
            "Val Loss: 0.9680, Val Acc: 0.1970, Val F1 Macro: 0.1367, Val F1 Micro: 0.2038\n",
            "Test Loss: 0.9679, Test Acc: 0.2305, Test F1 Macro: 0.1596, Test F1 Micro: 0.2305\n",
            "Epoch [7/30], Time: 11.49s, \n",
            "Train Loss: 0.9627, Train Acc: 0.2354, Train F1 Macro: 0.1425, Train F1 Micro: 0.2076, \n",
            "Val Loss: 0.9917, Val Acc: 0.2002, Val F1 Macro: 0.1373, Val F1 Micro: 0.2033\n",
            "Test Loss: 0.9648, Test Acc: 0.2218, Test F1 Macro: 0.1020, Test F1 Micro: 0.2218\n",
            "Epoch [8/30], Time: 11.63s, \n",
            "Train Loss: 0.9601, Train Acc: 0.2539, Train F1 Macro: 0.1517, Train F1 Micro: 0.2134, \n",
            "Val Loss: 0.9914, Val Acc: 0.2017, Val F1 Macro: 0.1450, Val F1 Micro: 0.2031\n",
            "Test Loss: 0.9645, Test Acc: 0.2399, Test F1 Macro: 0.1717, Test F1 Micro: 0.2399\n",
            "Epoch [9/30], Time: 11.87s, \n",
            "Train Loss: 0.9541, Train Acc: 0.2812, Train F1 Macro: 0.1630, Train F1 Micro: 0.2209, \n",
            "Val Loss: 1.0058, Val Acc: 0.2251, Val F1 Macro: 0.1503, Val F1 Micro: 0.2055\n",
            "Test Loss: 0.9683, Test Acc: 0.2289, Test F1 Macro: 0.1582, Test F1 Micro: 0.2289\n",
            "Epoch [10/30], Time: 12.03s, \n",
            "Train Loss: 0.9448, Train Acc: 0.3103, Train F1 Macro: 0.1772, Train F1 Micro: 0.2298, \n",
            "Val Loss: 1.0087, Val Acc: 0.2095, Val F1 Macro: 0.1596, Val F1 Micro: 0.2059\n",
            "Test Loss: 0.9670, Test Acc: 0.2320, Test F1 Macro: 0.2024, Test F1 Micro: 0.2320\n",
            "Epoch [11/30], Time: 11.80s, \n",
            "Train Loss: 0.9306, Train Acc: 0.3603, Train F1 Macro: 0.1968, Train F1 Micro: 0.2417, \n",
            "Val Loss: 1.0364, Val Acc: 0.2235, Val F1 Macro: 0.1650, Val F1 Micro: 0.2075\n",
            "Test Loss: 0.9752, Test Acc: 0.2257, Test F1 Macro: 0.1627, Test F1 Micro: 0.2257\n",
            "Epoch [12/30], Time: 11.74s, \n",
            "Train Loss: 0.9134, Train Acc: 0.4188, Train F1 Macro: 0.2189, Train F1 Micro: 0.2565, \n",
            "Val Loss: 1.0646, Val Acc: 0.2002, Val F1 Macro: 0.1722, Val F1 Micro: 0.2069\n",
            "Test Loss: 0.9844, Test Acc: 0.2297, Test F1 Macro: 0.2040, Test F1 Micro: 0.2297\n",
            "Epoch [13/30], Time: 11.75s, \n",
            "Train Loss: 0.8914, Train Acc: 0.4792, Train F1 Macro: 0.2429, Train F1 Micro: 0.2736, \n",
            "Val Loss: 1.0774, Val Acc: 0.1986, Val F1 Macro: 0.1754, Val F1 Micro: 0.2063\n",
            "Test Loss: 0.9993, Test Acc: 0.2344, Test F1 Macro: 0.2114, Test F1 Micro: 0.2344\n",
            "Epoch [14/30], Time: 11.77s, \n",
            "Train Loss: 0.8635, Train Acc: 0.5477, Train F1 Macro: 0.2682, Train F1 Micro: 0.2932, \n",
            "Val Loss: 1.0942, Val Acc: 0.2095, Val F1 Macro: 0.1795, Val F1 Micro: 0.2065\n",
            "Test Loss: 1.0328, Test Acc: 0.2234, Test F1 Macro: 0.2069, Test F1 Micro: 0.2234\n",
            "Epoch [15/30], Time: 11.78s, \n",
            "Train Loss: 0.8305, Train Acc: 0.6070, Train F1 Macro: 0.2938, Train F1 Micro: 0.3141, \n",
            "Val Loss: 1.1009, Val Acc: 0.2002, Val F1 Macro: 0.1821, Val F1 Micro: 0.2061\n",
            "Test Loss: 1.0460, Test Acc: 0.2281, Test F1 Macro: 0.2150, Test F1 Micro: 0.2281\n",
            "Epoch [16/30], Time: 11.78s, \n",
            "Train Loss: 0.8001, Train Acc: 0.6596, Train F1 Macro: 0.3190, Train F1 Micro: 0.3357, \n",
            "Val Loss: 1.1179, Val Acc: 0.2079, Val F1 Macro: 0.1854, Val F1 Micro: 0.2062\n",
            "Test Loss: 1.0616, Test Acc: 0.2202, Test F1 Macro: 0.2041, Test F1 Micro: 0.2202\n",
            "Epoch [17/30], Time: 11.82s, \n",
            "Train Loss: 0.7772, Train Acc: 0.6958, Train F1 Macro: 0.3430, Train F1 Micro: 0.3569, \n",
            "Val Loss: 1.1094, Val Acc: 0.2212, Val F1 Macro: 0.1882, Val F1 Micro: 0.2071\n",
            "Test Loss: 1.0594, Test Acc: 0.2368, Test F1 Macro: 0.2204, Test F1 Micro: 0.2368\n",
            "Epoch [18/30], Time: 12.22s, \n",
            "Train Loss: 0.7580, Train Acc: 0.7279, Train F1 Macro: 0.3658, Train F1 Micro: 0.3775, \n",
            "Val Loss: 1.1305, Val Acc: 0.2072, Val F1 Macro: 0.1893, Val F1 Micro: 0.2071\n",
            "Test Loss: 1.0737, Test Acc: 0.2415, Test F1 Macro: 0.2171, Test F1 Micro: 0.2415\n",
            "Epoch [19/30], Time: 12.22s, \n",
            "Train Loss: 0.7401, Train Acc: 0.7547, Train F1 Macro: 0.3874, Train F1 Micro: 0.3973, \n",
            "Val Loss: 1.1251, Val Acc: 0.2040, Val F1 Macro: 0.1906, Val F1 Micro: 0.2069\n",
            "Test Loss: 1.0716, Test Acc: 0.2352, Test F1 Macro: 0.2209, Test F1 Micro: 0.2352\n",
            "Epoch [20/30], Time: 12.28s, \n",
            "Train Loss: 0.7281, Train Acc: 0.7747, Train F1 Macro: 0.4076, Train F1 Micro: 0.4162, \n",
            "Val Loss: 1.1372, Val Acc: 0.2142, Val F1 Macro: 0.1917, Val F1 Micro: 0.2073\n",
            "Test Loss: 1.0805, Test Acc: 0.2360, Test F1 Macro: 0.2155, Test F1 Micro: 0.2360\n",
            "Epoch [21/30], Time: 12.07s, \n",
            "Train Loss: 0.7190, Train Acc: 0.7903, Train F1 Macro: 0.4265, Train F1 Micro: 0.4340, \n",
            "Val Loss: 1.1299, Val Acc: 0.2181, Val F1 Macro: 0.1935, Val F1 Micro: 0.2078\n",
            "Test Loss: 1.0869, Test Acc: 0.2249, Test F1 Macro: 0.2155, Test F1 Micro: 0.2249\n",
            "Epoch [22/30], Time: 11.97s, \n",
            "Train Loss: 0.7163, Train Acc: 0.7926, Train F1 Macro: 0.4435, Train F1 Micro: 0.4503, \n",
            "Val Loss: 1.1231, Val Acc: 0.2235, Val F1 Macro: 0.1948, Val F1 Micro: 0.2085\n",
            "Test Loss: 1.0819, Test Acc: 0.2242, Test F1 Macro: 0.2063, Test F1 Micro: 0.2242\n",
            "Epoch [23/30], Time: 11.98s, \n",
            "Train Loss: 0.7110, Train Acc: 0.8009, Train F1 Macro: 0.4594, Train F1 Micro: 0.4656, \n",
            "Val Loss: 1.1420, Val Acc: 0.2009, Val F1 Macro: 0.1947, Val F1 Micro: 0.2082\n",
            "Test Loss: 1.0836, Test Acc: 0.2391, Test F1 Macro: 0.2218, Test F1 Micro: 0.2391\n",
            "Epoch [24/30], Time: 12.03s, \n",
            "Train Loss: 0.7061, Train Acc: 0.8098, Train F1 Macro: 0.4742, Train F1 Micro: 0.4799, \n",
            "Val Loss: 1.1358, Val Acc: 0.2103, Val F1 Macro: 0.1953, Val F1 Micro: 0.2083\n",
            "Test Loss: 1.0811, Test Acc: 0.2265, Test F1 Macro: 0.2070, Test F1 Micro: 0.2265\n",
            "Epoch [25/30], Time: 12.01s, \n",
            "Train Loss: 0.7040, Train Acc: 0.8132, Train F1 Macro: 0.4878, Train F1 Micro: 0.4932, \n",
            "Val Loss: 1.1401, Val Acc: 0.2017, Val F1 Macro: 0.1956, Val F1 Micro: 0.2080\n",
            "Test Loss: 1.0812, Test Acc: 0.2305, Test F1 Macro: 0.2118, Test F1 Micro: 0.2305\n",
            "Epoch [26/30], Time: 12.01s, \n",
            "Train Loss: 0.7048, Train Acc: 0.8116, Train F1 Macro: 0.5003, Train F1 Micro: 0.5055, \n",
            "Val Loss: 1.1532, Val Acc: 0.1869, Val F1 Macro: 0.1953, Val F1 Micro: 0.2072\n",
            "Test Loss: 1.0874, Test Acc: 0.2234, Test F1 Macro: 0.2052, Test F1 Micro: 0.2234\n",
            "Epoch [27/30], Time: 12.09s, \n",
            "Train Loss: 0.7006, Train Acc: 0.8168, Train F1 Macro: 0.5119, Train F1 Micro: 0.5170, \n",
            "Val Loss: 1.1347, Val Acc: 0.2087, Val F1 Macro: 0.1956, Val F1 Micro: 0.2073\n",
            "Test Loss: 1.0857, Test Acc: 0.2249, Test F1 Macro: 0.2068, Test F1 Micro: 0.2249\n",
            "Epoch [28/30], Time: 12.11s, \n",
            "Train Loss: 0.7009, Train Acc: 0.8188, Train F1 Macro: 0.5228, Train F1 Micro: 0.5278, \n",
            "Val Loss: 1.1547, Val Acc: 0.1861, Val F1 Macro: 0.1954, Val F1 Micro: 0.2065\n",
            "Test Loss: 1.0837, Test Acc: 0.2289, Test F1 Macro: 0.2119, Test F1 Micro: 0.2289\n",
            "Epoch [29/30], Time: 12.16s, \n",
            "Train Loss: 0.6998, Train Acc: 0.8171, Train F1 Macro: 0.5328, Train F1 Micro: 0.5378, \n",
            "Val Loss: 1.1395, Val Acc: 0.2056, Val F1 Macro: 0.1954, Val F1 Micro: 0.2065\n",
            "Test Loss: 1.0789, Test Acc: 0.2344, Test F1 Macro: 0.2171, Test F1 Micro: 0.2344\n",
            "Epoch [30/30], Time: 12.59s, \n",
            "Train Loss: 0.6979, Train Acc: 0.8208, Train F1 Macro: 0.5422, Train F1 Micro: 0.5472, \n",
            "Val Loss: 1.1485, Val Acc: 0.1916, Val F1 Macro: 0.1954, Val F1 Micro: 0.2060\n",
            "Test Loss: 1.0853, Test Acc: 0.2273, Test F1 Macro: 0.2107, Test F1 Micro: 0.2273\n",
            "Total Training Time: 372.08s\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 30522\n",
        "\n",
        "embedding_dim = 128\n",
        "n_filters = 128\n",
        "filter_sizes = [3,4,5]\n",
        "output_dim = 6\n",
        "dropout = 0.5\n",
        "padding_idx = 0\n",
        "input_dim = 6 * metadata_each_dim\n",
        "input_dim_metadata = 5\n",
        "hidden_dim = 64\n",
        "n_layers = 1\n",
        "bidirectional = True\n",
        "\n",
        "model = LiarModel(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, padding_idx, input_dim, input_dim_metadata, hidden_dim, n_layers, bidirectional).to(DEVICE)\n",
        "\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "# Record the training process\n",
        "Train_acc = []\n",
        "Train_loss = []\n",
        "Train_macro_f1 = []\n",
        "Train_micro_f1 = []\n",
        "\n",
        "Val_acc = []\n",
        "Val_loss = []\n",
        "Val_macro_f1 = []\n",
        "Val_micro_f1 = []\n",
        "\n",
        "# Evaluate the model on new data\n",
        "def test(model, test_loader, model_save):\n",
        "    model.load_state_dict(torch.load(model_save))\n",
        "    model.eval()\n",
        "\n",
        "    test_label_all = []\n",
        "    test_predict_all = []\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for statements, label_onehot, label, metadata_text, metadata_number in test_loader:\n",
        "            statements = statements.to(DEVICE)\n",
        "            label_onehot = label_onehot.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            metadata_text = metadata_text.to(DEVICE)\n",
        "            metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "            test_outputs = model(statements, metadata_text, metadata_number)\n",
        "            test_loss += criterion(test_outputs, label_onehot).item()\n",
        "            _, test_predicted = torch.max(test_outputs, 1)\n",
        "\n",
        "            test_accuracy += sum(test_predicted == label)\n",
        "            test_predict_all += test_predicted.tolist()\n",
        "            test_label_all += label.tolist()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy /= len(test_loader.dataset)\n",
        "    test_macro_f1 = f1_score(test_label_all, test_predict_all, average='macro')\n",
        "    test_micro_f1 = f1_score(test_label_all, test_predict_all, average='micro')\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}, Test F1 Macro: {test_macro_f1:.4f}, Test F1 Micro: {test_micro_f1:.4f}')\n",
        "\n",
        "def train(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_save):\n",
        "    epoch_trained = 0\n",
        "    train_label_all = []\n",
        "    train_predict_all = []\n",
        "    val_label_all = []\n",
        "    val_predict_all = []\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_trained += 1\n",
        "        epoch_start_time = time.time()\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        for statements, label_onehot, label, metadata_text, metadata_number in train_loader:\n",
        "            statements = statements.to(DEVICE)\n",
        "            label_onehot = label_onehot.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            metadata_text = metadata_text.to(DEVICE)\n",
        "            metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(statements, metadata_text, metadata_number)\n",
        "\n",
        "            loss = criterion(outputs, label_onehot)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, train_predicted = torch.max(outputs, 1)\n",
        "            train_accuracy += sum(train_predicted == label)\n",
        "            train_predict_all += train_predicted.tolist()\n",
        "            train_label_all += label.tolist()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy /= len(train_loader.dataset)\n",
        "        train_macro_f1 = f1_score(train_label_all, train_predict_all, average='macro')\n",
        "        train_micro_f1 = f1_score(train_label_all, train_predict_all, average='micro')\n",
        "\n",
        "        Train_acc.append(train_accuracy.tolist())\n",
        "        Train_loss.append(train_loss)\n",
        "        Train_macro_f1.append(train_macro_f1)\n",
        "        Train_micro_f1.append(train_micro_f1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for statements, label_onehot, label, metadata_text, metadata_number in val_loader:\n",
        "                statements = statements.to(DEVICE)\n",
        "                label_onehot = label_onehot.to(DEVICE)\n",
        "                label = label.to(DEVICE)\n",
        "                metadata_text = metadata_text.to(DEVICE)\n",
        "                metadata_number = metadata_number.to(DEVICE)\n",
        "\n",
        "                val_outputs = model(statements, metadata_text, metadata_number)\n",
        "                val_loss += criterion(val_outputs, label_onehot).item()\n",
        "                _, val_predicted = torch.max(val_outputs, 1)\n",
        "                val_accuracy += sum(val_predicted == label)\n",
        "                val_predict_all += val_predicted.tolist()\n",
        "                val_label_all += label.tolist()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader.dataset)\n",
        "        val_macro_f1 = f1_score(val_label_all, val_predict_all, average='macro')\n",
        "        val_micro_f1 = f1_score(val_label_all, val_predict_all, average='micro')\n",
        "\n",
        "        Val_acc.append(val_accuracy.tolist())\n",
        "        Val_loss.append(val_loss)\n",
        "        Val_macro_f1.append(val_macro_f1)\n",
        "        Val_micro_f1.append(val_micro_f1)\n",
        "\n",
        "        # if val_loss < best_valid_loss:\n",
        "        #     best_valid_loss = val_loss\n",
        "        #     torch.save(model.state_dict(), model_save)\n",
        "        #     print(f'***** Best Result Updated at Epoch {epoch_trained}, Val Loss: {val_loss:.4f} *****')\n",
        "\n",
        "        if val_loss < best_valid_loss:\n",
        "            best_valid_loss = val_loss\n",
        "            print(f'***** Best Result Updated at Epoch {epoch_trained}, Val Loss: {val_loss:.4f} *****')\n",
        "\n",
        "        torch.save(model.state_dict(), model_save)\n",
        "\n",
        "        # Print the losses and accuracy\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f}s, \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1 Macro: {train_macro_f1:.4f}, Train F1 Micro: {train_micro_f1:.4f}, \\nVal Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1 Macro: {val_macro_f1:.4f}, Val F1 Micro: {val_micro_f1:.4f}\")\n",
        "\n",
        "        test(model, test_loader, model_save)\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f'Total Training Time: {training_time:.2f}s')\n",
        "\n",
        "\n",
        "train(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_save)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPjVzzDboY3_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTh9SswgoYr8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}