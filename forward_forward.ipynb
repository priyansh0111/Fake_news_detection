{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJSUOuRm7Hh",
        "outputId": "3014af9a-7529-4cba-a5d0-afee0e0ac8cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "b-IHznJ8mS5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing the randomness of CUDA.\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"PyTorch Version : {}\".format(torch.__version__))\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "uHLmItSbm6EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6c599f-6154-466f-bc01-00ac12ee5d3b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version : 2.2.1+cu121\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worksapce = '/content/drive/MyDrive/fasttext/'\n",
        "model_save = 'TC+TC+CB+FZ.pt'\n",
        "model_name = 'TC+TC+CB+FZ'\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "num_classes = 6\n",
        "padding_idx = 0\n",
        "metadata_each_dim = 10\n",
        "\n",
        "\n",
        "col = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "\n",
        "label_map = {0: 'pants-fire', 1: 'false', 2: 'barely-true', 3: 'half-true', 4: 'mostly-true', 5: 'true'}\n",
        "label_convert = {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true':5}\n"
      ],
      "metadata": {
        "id": "28yIwHQvnNxG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(worksapce + 'train.tsv', sep = '\\t', names = col)\n",
        "test_data = pd.read_csv(worksapce + 'test.tsv', sep = '\\t', names = col)\n",
        "val_data = pd.read_csv(worksapce + 'valid.tsv', sep = '\\t', names = col)\n",
        "\n",
        "# Replace NaN values with 'NaN'\n",
        "train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = train_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "train_data.fillna('NaN', inplace=True)\n",
        "\n",
        "test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = test_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "test_data.fillna('NaN', inplace=True)\n",
        "\n",
        "val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']] = val_data[['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']].fillna(0)\n",
        "val_data.fillna('NaN', inplace=True)"
      ],
      "metadata": {
        "id": "_qd4BRconZME"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# val_data = pd.concat([train_data.iloc[0:1], val_data]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "cN1kCmUUwCzy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "SSjFb2DlxNWL",
        "outputId": "809780a1-9550-4581-f1d7-ec49a852f322"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id        label                                          statement  \\\n",
              "0  12134.json  barely-true  We have less Americans working now than in the...   \n",
              "\n",
              "        subject         speaker            job_title state_info  \\\n",
              "0  economy,jobs  vicky-hartzler  U.S. Representative   Missouri   \n",
              "\n",
              "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
              "0        republican                   1             0                 1   \n",
              "\n",
              "   mostly_true_counts  pants_on_fire_counts                       context  \n",
              "0                   0                     0  an interview with ABC17 News  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea77cc9b-fccf-4c97-b3d6-af9af54c4eea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>statement</th>\n",
              "      <th>subject</th>\n",
              "      <th>speaker</th>\n",
              "      <th>job_title</th>\n",
              "      <th>state_info</th>\n",
              "      <th>party_affiliation</th>\n",
              "      <th>barely_true_counts</th>\n",
              "      <th>false_counts</th>\n",
              "      <th>half_true_counts</th>\n",
              "      <th>mostly_true_counts</th>\n",
              "      <th>pants_on_fire_counts</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12134.json</td>\n",
              "      <td>barely-true</td>\n",
              "      <td>We have less Americans working now than in the...</td>\n",
              "      <td>economy,jobs</td>\n",
              "      <td>vicky-hartzler</td>\n",
              "      <td>U.S. Representative</td>\n",
              "      <td>Missouri</td>\n",
              "      <td>republican</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>an interview with ABC17 News</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea77cc9b-fccf-4c97-b3d6-af9af54c4eea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea77cc9b-fccf-4c97-b3d6-af9af54c4eea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea77cc9b-fccf-4c97-b3d6-af9af54c4eea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"val_data[0:1]\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"12134.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"barely-true\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"We have less Americans working now than in the 70s.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"economy,jobs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"vicky-hartzler\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"U.S. Representative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state_info\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Missouri\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"party_affiliation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"republican\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"barely_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"false_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"half_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mostly_true_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pants_on_fire_counts\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"an interview with ABC17 News\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def textProcess(input_text, max_length = 512):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    if True and max_length == -1:\n",
        "        tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "    else:\n",
        "        tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "nIF8FRGbncKA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import XLNetTokenizer\n",
        "\n",
        "# def textProcess(input_text, max_length=-1):\n",
        "#   tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')  # Adjust casing based on your needs\n",
        "#   if max_length == -1:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "#   else:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "#   return tokens"
      ],
      "metadata": {
        "id": "HbV2T6y4qEGu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import XLNetTokenizer\n",
        "\n",
        "# def textProcess(input_text, max_length=-1):\n",
        "#   tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')  # Adjust casing based on your needs\n",
        "#   if max_length == -1:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding=True)\n",
        "#   else:\n",
        "#     tokens = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length)\n",
        "#   return tokens"
      ],
      "metadata": {
        "id": "mt4ZD0YfYMsg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset for loading the data\n",
        "class LiarDataset(data.Dataset):\n",
        "    def __init__(self, data_df, statement, label_onehot, label, subject, speaker, job_title, state_info,\n",
        "                     party_affiliation, barely_true_counts, false_counts, half_true_counts, mostly_true_counts,\n",
        "                    pants_on_fire_counts, context):\n",
        "        self.data_df = data_df\n",
        "        self.statement = statement\n",
        "        self.label_onehot = label_onehot\n",
        "        self.label = label\n",
        "        self.metadata_text = torch.cat((subject.int(), speaker.int(), job_title.int(), state_info.int(), party_affiliation.int(),\n",
        "                                   context.int()), dim=-1)\n",
        "        self.metadata_number = torch.cat((torch.tensor(barely_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(false_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(half_true_counts, dtype=torch.float).unsqueeze(1), torch.tensor(mostly_true_counts, dtype=torch.float).unsqueeze(1),\n",
        "                                   torch.tensor(pants_on_fire_counts, dtype=torch.float).unsqueeze(1)), dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        statement = self.statement[idx]\n",
        "        label_onehot = self.label_onehot[idx]\n",
        "        label = self.label[idx]\n",
        "        metadata_text = self.metadata_text[idx]\n",
        "        metadata_number = self.metadata_number[idx]\n",
        "        return statement, label_onehot, label, metadata_text, metadata_number"
      ],
      "metadata": {
        "id": "WIw-WJlWWr3H"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data loaders for training and validation\n",
        "train_text = torch.tensor(textProcess(train_data['statement'].tolist())['input_ids'])\n",
        "train_label = torch.nn.functional.one_hot(torch.tensor(train_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "train_subject = torch.tensor(textProcess(train_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_speaker = torch.tensor(textProcess(train_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_job_title = torch.tensor(textProcess(train_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_state_info = torch.tensor(textProcess(train_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_party_affiliation = torch.tensor(textProcess(train_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "train_context = torch.tensor(textProcess(train_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "train_dataset = LiarDataset(train_data, train_text, train_label, torch.tensor(train_data['label'].replace(label_convert)),\n",
        "                            train_subject, train_speaker, train_job_title,\n",
        "                            train_state_info, train_party_affiliation,\n",
        "                            train_data['barely_true_counts'].tolist(), train_data['false_counts'].tolist(),\n",
        "                            train_data['half_true_counts'].tolist(), train_data['mostly_true_counts'].tolist(),\n",
        "                            train_data['pants_on_fire_counts'].tolist(), train_context)\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_text = torch.tensor(textProcess(val_data['statement'].tolist())['input_ids'])\n",
        "val_label = torch.nn.functional.one_hot(torch.tensor(val_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "val_subject = torch.tensor(textProcess(val_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_speaker = torch.tensor(textProcess(val_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_job_title = torch.tensor(textProcess(val_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_state_info = torch.tensor(textProcess(val_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_party_affiliation = torch.tensor(textProcess(val_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "val_context = torch.tensor(textProcess(val_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "val_dataset = LiarDataset(val_data, val_text, val_label, torch.tensor(val_data['label'].replace(label_convert)),\n",
        "                          val_subject, val_speaker, val_job_title,\n",
        "                          val_state_info, val_party_affiliation,\n",
        "                          val_data['barely_true_counts'].tolist(), val_data['false_counts'].tolist(),\n",
        "                          val_data['half_true_counts'].tolist(), val_data['mostly_true_counts'].tolist(),\n",
        "                          val_data['pants_on_fire_counts'].tolist(), val_context)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "test_text = torch.tensor(textProcess(test_data['statement'].tolist())['input_ids'])\n",
        "test_label = torch.nn.functional.one_hot(torch.tensor(test_data['label'].replace(label_convert)), num_classes=6).type(torch.float64)\n",
        "test_subject = torch.tensor(textProcess(test_data['subject'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_speaker = torch.tensor(textProcess(test_data['speaker'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_job_title = torch.tensor(textProcess(test_data['job_title'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_state_info = torch.tensor(textProcess(test_data['state_info'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_party_affiliation = torch.tensor(textProcess(test_data['party_affiliation'].tolist(), metadata_each_dim)['input_ids'])\n",
        "test_context = torch.tensor(textProcess(test_data['context'].tolist(), metadata_each_dim)['input_ids'])\n",
        "\n",
        "test_dataset = LiarDataset(test_data, test_text, test_label, torch.tensor(test_data['label'].replace(label_convert)),\n",
        "                          test_subject, test_speaker, test_job_title,\n",
        "                          test_state_info, test_party_affiliation,\n",
        "                          test_data['barely_true_counts'].tolist(), test_data['false_counts'].tolist(),\n",
        "                          test_data['half_true_counts'].tolist(), test_data['mostly_true_counts'].tolist(),\n",
        "                          test_data['pants_on_fire_counts'].tolist(), test_context)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "AF7lM401WuZM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['statement'].to_list()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8JpET_MbudyL",
        "outputId": "1da6ca18-93c7-493d-97a2-db91cbb63c56"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Says the Annies List political group supports third-trimester abortions on demand.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(textProcess(val_data['statement'].tolist())['input_ids']).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDUbzNpttCWI",
        "outputId": "bdadfca6-1e5a-4276-dc4c-4656fa6c1bb4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1284, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(val_data['label'].replace(label_convert))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8vVn3rdkbgl",
        "outputId": "2d83bb8f-cb9e-45e2-f659-deab54103b3f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 0, 1,  ..., 5, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Forward 2"
      ],
      "metadata": {
        "id": "YNb3AhcRi-EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text2 = train_text.float()\n",
        "test_text2 = test_text.float()\n",
        "val_text2 = val_text.float()\n"
      ],
      "metadata": {
        "id": "piOraO9CprSz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(train_text2, torch.tensor(train_data['label'].replace(label_convert)))\n",
        "test_dataset = TensorDataset(test_text2, torch.tensor(test_data['label'].replace(label_convert)))\n",
        "val_dataset = TensorDataset(val_text2, torch.tensor(val_data['label'].replace(label_convert)))"
      ],
      "metadata": {
        "id": "TJ952WsvjC4Y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_text.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xQBTeGYj6WA",
        "outputId": "cb4ac0fc-45ae-42f0-d641-150ab04ba545"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1284, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def DATA_loaders(train_batch_size=1000, test_batch_size=10000):\n",
        "\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,)),\n",
        "        Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    eval_train_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    eval_test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, eval_train_loader, eval_test_loader\n",
        "\n",
        "def create_data_pos(images, labels):\n",
        "    return overlay_labels_on_images(images, labels)\n",
        "\n",
        "def create_data_neg(images, labels):\n",
        "    labels_neg = labels.clone()\n",
        "    for idx, y in enumerate(labels):\n",
        "        all_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "        all_labels.pop(y.item()) # remove y from labels to generate negative data\n",
        "        labels_neg[idx] = torch.tensor(np.random.choice(all_labels)).cpu()\n",
        "    return overlay_labels_on_images(images, labels_neg)\n",
        "\n",
        "def overlay_labels_on_images(images, labels):\n",
        "    \"\"\"Replace the first 10 pixels of images with one-hot-encoded labels\n",
        "    \"\"\"\n",
        "    num_images = images.shape[0]\n",
        "    data = images.clone()\n",
        "    data[:, :10] *= 0.0\n",
        "    data[range(0,num_images), labels] = images.max()\n",
        "    return data\n",
        "\n",
        "def visualize_sample(data, name='', idx=0):\n",
        "    reshaped = data[idx].cpu().reshape(28, 28)\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.title(name)\n",
        "    plt.imshow(reshaped, cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F1ygb946jEXq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "\n",
        "class FFNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.num_epochs = 20\n",
        "        self.layers = []\n",
        "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = 'cpu'\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers += [FFLayer(dims[d], dims[d + 1]).cpu()]\n",
        "\n",
        "    \"\"\"\n",
        "    There are two approaches for batch training:\n",
        "    1. Iterate batches for all layers. ---> easy\n",
        "    2. Iterate batches for each layer. ---> need to create new batches for next layer input\n",
        "    We use 1 for the following two training methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def train_1(self, data_loader):\n",
        "        \"\"\"\n",
        "        Train method 1: train all layers for each epoch for each batch.\n",
        "        \"\"\"\n",
        "        for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
        "            print(\"Training Batch (Size:\", str(x_batch.size(dim=0)) + ')', '#', batch_i + 1, '/', len(data_loader))\n",
        "            batch_pos, batch_neg = create_data_pos(x_batch, y_batch), create_data_neg(x_batch, y_batch)\n",
        "            batch_pos, batch_neg = batch_pos.to(self.device), batch_neg.to(self.device)\n",
        "            for epoch in tqdm(range(self.num_epochs)):\n",
        "                h_batch_pos, h_batch_neg = batch_pos, batch_neg\n",
        "                for layer_i, layer in enumerate(self.layers):\n",
        "                    h_batch_pos, h_batch_neg, loss = layer.train(h_batch_pos, h_batch_neg)\n",
        "\n",
        "            print('train error:', str(round((1.0 - self.predict(eval_train_loader)) * 100, 2)) + '%')\n",
        "            print('test error:', str(round((1.0 - self.predict(eval_test_loader)) * 100, 2)) + '%')\n",
        "\n",
        "\n",
        "    def train_2(self, data_loader):\n",
        "        \"\"\"\n",
        "        Train method 2: train all epochs for each layer for each batch.\n",
        "        \"\"\"\n",
        "        for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
        "            batch_loss = 0\n",
        "            print(\"Training Batch (Size:\", str(x_batch.size(dim=0)) + ')', '#', batch_i + 1, '/', len(data_loader))\n",
        "            h_batch_pos, h_batch_neg = create_data_pos(x_batch, y_batch), create_data_neg(x_batch, y_batch)\n",
        "            h_batch_pos, h_batch_neg = h_batch_pos.to(self.device), h_batch_neg.to(self.device)\n",
        "            for layer_i, layer in enumerate(tqdm(self.layers)):\n",
        "                for epoch in range(self.num_epochs):\n",
        "                    h_batch_pos_epoch, h_batch_neg_epoch, loss = layer.train(h_batch_pos, h_batch_neg)\n",
        "                    batch_loss += loss.item()\n",
        "                h_batch_pos, h_batch_neg = h_batch_pos_epoch, h_batch_neg_epoch\n",
        "\n",
        "            print('train error:', str(round((1.0 - self.predict(eval_train_loader)) * 100, 2)) + '%')\n",
        "            print('test error:', str(round((1.0 - self.predict(eval_test_loader)) * 100, 2)) + '%')\n",
        "\n",
        "            print('batch {} loss: {}'.format(batch_i + 1, batch_loss))\n",
        "\n",
        "    def train_3(self, data_loader):\n",
        "        \"\"\"\n",
        "        Train method 3: train all layers for each batch for each epoch. [Slow but better?]\n",
        "        \"\"\"\n",
        "        cached_data = []\n",
        "        for epoch in tqdm(range(self.num_epochs)):\n",
        "            epoch_loss = 0\n",
        "            for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
        "                # print(\"Training Batch (Size:\", str(x_batch.size(dim=0)) + ')', '#', batch_i + 1, '/', len(data_loader))\n",
        "                if (epoch + 1) == 1:\n",
        "                    h_batch_pos, h_batch_neg = create_data_pos(x_batch, y_batch), create_data_neg(x_batch, y_batch)\n",
        "                    h_batch_pos, h_batch_neg = h_batch_pos.to(self.device), h_batch_neg.to(self.device)\n",
        "                    cached_data.append((h_batch_pos, h_batch_neg))\n",
        "                else:\n",
        "                    h_batch_pos, h_batch_neg = cached_data[batch_i]\n",
        "                for layer_i, layer in enumerate(self.layers):\n",
        "                    h_batch_pos_epoch, h_batch_neg_epoch, loss = layer.train(h_batch_pos, h_batch_neg)\n",
        "                    epoch_loss += loss.item()\n",
        "                    h_batch_pos, h_batch_neg = h_batch_pos_epoch, h_batch_neg_epoch\n",
        "\n",
        "            print('train error:', str(round((1.0 - self.predict(eval_train_loader)) * 100, 2)) + '%')\n",
        "            print('test error:', str(round((1.0 - self.predict(eval_test_loader)) * 100, 2)) + '%')\n",
        "\n",
        "            print('   epoch {} loss: {}'.format(epoch + 1, epoch_loss))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, data_loader):\n",
        "        all_predictions = torch.Tensor([])\n",
        "        all_labels = torch.Tensor([])\n",
        "        all_predictions, all_labels = all_predictions.to(self.device), all_labels.to(self.device)\n",
        "        for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
        "            print(\"Evaluation Batch (Size:\", str(x_batch.size(dim=0)) + ')', '#', batch_i + 1, '/', len(data_loader))\n",
        "            x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "            goodness_per_label_batch = []\n",
        "            for label in range(10):\n",
        "                h_batch = overlay_labels_on_images(x_batch, label)\n",
        "                goodness_batch = []\n",
        "                for layer in self.layers:\n",
        "                    h_batch = layer(h_batch)\n",
        "                    goodness_batch += [h_batch.pow(2).mean(1)]\n",
        "                goodness_per_label_batch += [sum(goodness_batch).unsqueeze(1)]\n",
        "            goodness_per_label_batch = torch.cat(goodness_per_label_batch, 1)\n",
        "            predictions_batch = goodness_per_label_batch.argmax(1)\n",
        "            all_predictions = torch.cat((all_predictions, predictions_batch), 0)\n",
        "            all_labels = torch.cat((all_labels, y_batch), 0)\n",
        "        return all_predictions.eq(all_labels).float().mean().item()\n",
        "\n",
        "\n",
        "class FFLayer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.leakyrelu = torch.nn.LeakyReLU()\n",
        "        self.rrelu = torch.nn.RReLU()\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.opt = torch.optim.AdamW(self.parameters(), lr=0.02)\n",
        "        self.threshold = 2.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "        g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "        # The following loss pushes pos (neg) samples to values larger (smaller) than the self.threshold.\n",
        "        loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "        self.opt.zero_grad()\n",
        "        # this backward just compute the derivative and hence is not considered backpropagation.\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach(), loss.detach()\n"
      ],
      "metadata": {
        "id": "ft0JpIHSjKkt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "import time\n",
        "train_loader, eval_train_loader, eval_test_loader = DATA_loaders()\n",
        "\n",
        "net = FFNet([512, 2000, 2000, 2000, 2000, 6])\n",
        "\n",
        "time_training_start = time.time()\n",
        "net.train_3(train_loader)\n",
        "# net.train_2(train_loader)\n",
        "# net.train_1(train_loader)\n",
        "time_training_end = time.time()\n",
        "training_time = round(time_training_end - time_training_start, 2)\n",
        "\n",
        "print(f\"Training time: {training_time}s\")\n",
        "\n",
        "print('train error:', str(round((1.0 - net.predict(eval_train_loader)) * 100, 2)) + '%')\n",
        "\n",
        "print('test error:', str(round((1.0 - net.predict(eval_test_loader)) * 100, 2)) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "_GAt15DtjLHK",
        "outputId": "4f21f118-ead2-4f71-f8ae-73e54725c484"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Batch (Size: 1284) # 1 / 1\n",
            "train error: 79.52%\n",
            "Evaluation Batch (Size: 1267) # 1 / 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:40<12:57, 40.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test error: 80.35%\n",
            "   epoch 1 loss: 47.97023546695709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:43<13:47, 43.58s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0823f8ef88f4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtime_training_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# net.train_2(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# net.train_1(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-9416eb8a4929>\u001b[0m in \u001b[0;36mtrain_3\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mh_batch_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_batch_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mh_batch_pos_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_batch_neg_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_batch_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_batch_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mh_batch_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_batch_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_batch_pos_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_batch_neg_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-9416eb8a4929>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-9416eb8a4929>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mx_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ufbvK3yofwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}